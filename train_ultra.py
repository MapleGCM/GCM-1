"""
è¶…é«˜æ•ˆè®­ç»ƒè„šæœ¬ - ç”¨1ä¸‡è¡Œæ•°æ®è¾¾åˆ°100ä¸‡è¡Œæ•ˆæœ
Ultra Efficient Training Script

ä½¿ç”¨æ–¹æ³•:
python train_ultra.py --data_path data/1w.txt --epochs 100 --augmentation_factor 15
"""

import torch
import argparse
import os
import json
from datetime import datetime
from model import GCMLLM
from tokenizer import SimpleTokenizer
from data_loader import TextDataLoader
from ultra_efficient_trainer import ultra_efficient_train


def main():
    parser = argparse.ArgumentParser(description='Ultra Efficient Training - 1ä¸‡è¡Œè¾¾åˆ°100ä¸‡è¡Œæ•ˆæœ')
    
    parser.add_argument('--data_path', type=str, required=True,
                       help='Path to training data (file or directory)')
    parser.add_argument('--data_format', type=str, default='auto',
                       choices=['auto', 'txt', 'json', 'jsonl'])
    parser.add_argument('--text_key', type=str, default='text')
    parser.add_argument('--line_mode', action='store_true',
                       help='Treat each line as a separate sample')
    parser.add_argument('--min_length', type=int, default=5)
    parser.add_argument('--max_length', type=int, default=2000)
    
    parser.add_argument('--d_model', type=int, default=512)
    parser.add_argument('--n_heads', type=int, default=8)
    parser.add_argument('--n_layers', type=int, default=6)
    parser.add_argument('--d_ff', type=int, default=2048)
    parser.add_argument('--max_len', type=int, default=512)
    parser.add_argument('--dropout', type=float, default=0.1)
    
    parser.add_argument('--batch_size', type=int, default=8)
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--lr', type=float, default=0.0001)
    parser.add_argument('--val_split', type=float, default=0.1)
    
    parser.add_argument('--augmentation_factor', type=int, default=15,
                       help='æ•°æ®å¢å¼ºå€æ•°ï¼ˆæ¯ä¸ªæ ·æœ¬ç”Ÿæˆå¤šå°‘ä¸ªå¢å¼ºç‰ˆæœ¬ï¼‰')
    parser.add_argument('--use_amp', action='store_true', default=True,
                       help='Use mixed precision training')
    parser.add_argument('--use_rope', action='store_true', default=True,
                       help='Use RoPE position encoding')
    parser.add_argument('--use_flash', action='store_true',
                       help='Use Flash Attention')
    parser.add_argument('--use_gradient_checkpointing', action='store_true',
                       help='Use gradient checkpointing')
    
    parser.add_argument('--save_dir', type=str, default='checkpoints')
    parser.add_argument('--save_interval', type=int, default=10)
    parser.add_argument('--log_interval', type=int, default=50)
    
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸš€ è¶…é«˜æ•ˆè®­ç»ƒç³»ç»Ÿ")
    print("=" * 80)
    print(f"ç›®æ ‡: ç”¨ {args.data_path} çš„1ä¸‡è¡Œæ•°æ®è¾¾åˆ°100ä¸‡è¡Œæ•°æ®çš„æ•ˆæœ")
    print(f"æ•°æ®å¢å¼ºå€æ•°: {args.augmentation_factor}x")
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)
    
    print("\nğŸ“‚ åŠ è½½æ•°æ®...")
    
    use_line_mode = args.line_mode
    data_loader = TextDataLoader(
        min_length=args.min_length,
        max_length=args.max_length,
        line_mode=use_line_mode
    )
    
    if os.path.isfile(args.data_path):
        if args.data_format == 'auto':
            texts = data_loader.load_file(args.data_path, text_key=args.text_key)
        elif args.data_format == 'txt':
            texts = data_loader.load_txt(args.data_path)
        elif args.data_format == 'json':
            texts = data_loader.load_json(args.data_path, text_key=args.text_key)
        elif args.data_format == 'jsonl':
            texts = data_loader.load_jsonl(args.data_path, text_key=args.text_key)
        else:
            raise ValueError(f"Unsupported format: {args.data_format}")
    elif os.path.isdir(args.data_path):
        texts = data_loader.load_directory(args.data_path)
    else:
        raise ValueError(f"Data path not found: {args.data_path}")
    
    if len(texts) == 0 and not use_line_mode and os.path.isfile(args.data_path):
        print("\nâš ï¸  æ®µè½æ¨¡å¼åŠ è½½å¤±è´¥ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°è¡Œæ¨¡å¼...")
        use_line_mode = True
        data_loader = TextDataLoader(
            min_length=args.min_length,
            max_length=args.max_length,
            line_mode=True
        )
        
        if args.data_format == 'auto' or args.data_format == 'txt':
            texts = data_loader.load_txt(args.data_path)
        elif args.data_format == 'json':
            texts = data_loader.load_json(args.data_path, text_key=args.text_key)
        elif args.data_format == 'jsonl':
            texts = data_loader.load_jsonl(args.data_path, text_key=args.text_key)
        
        if len(texts) > 0:
            print(f"âœ“ è¡Œæ¨¡å¼æˆåŠŸåŠ è½½ {len(texts)} ä¸ªæ ·æœ¬")
        else:
            print("\nâš ï¸  å°è¯•æ”¾å®½é•¿åº¦é™åˆ¶...")
            data_loader = TextDataLoader(
                min_length=1,
                max_length=10000,
                line_mode=True
            )
            
            if args.data_format == 'auto' or args.data_format == 'txt':
                texts = data_loader.load_txt(args.data_path)
            elif args.data_format == 'json':
                texts = data_loader.load_json(args.data_path, text_key=args.text_key)
            elif args.data_format == 'jsonl':
                texts = data_loader.load_jsonl(args.data_path, text_key=args.text_key)
    
    if len(texts) == 0:
        print("\nâŒ é”™è¯¯: æ— æ³•åŠ è½½ä»»ä½•æœ‰æ•ˆæ•°æ®!")
        print("\nå¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥æ•°æ®æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®")
        print("2. ä½¿ç”¨ --line_mode å‚æ•°ï¼ˆå¦‚æœæ¯è¡Œæ˜¯ä¸€ä¸ªæ ·æœ¬ï¼‰")
        print("3. è°ƒæ•´ --min_length å’Œ --max_length å‚æ•°")
        print(f"   å½“å‰è®¾ç½®: min_length={args.min_length}, max_length={args.max_length}")
        print("4. æ£€æŸ¥æ•°æ®æ–‡ä»¶ç¼–ç æ˜¯å¦ä¸º UTF-8")
        raise ValueError("No valid texts loaded!")
    
    print(f"âœ“ åŠ è½½äº† {len(texts)} ä¸ªè®­ç»ƒæ ·æœ¬")
    if len(texts) > 0:
        avg_len = sum(len(t) for t in texts) / len(texts)
        min_len = min(len(t) for t in texts)
        max_len = max(len(t) for t in texts)
        print(f"  å¹³å‡é•¿åº¦: {avg_len:.1f} å­—ç¬¦")
        print(f"  é•¿åº¦èŒƒå›´: {min_len} - {max_len} å­—ç¬¦")
        print(f"  ä½¿ç”¨æ¨¡å¼: {'è¡Œæ¨¡å¼' if use_line_mode else 'æ®µè½æ¨¡å¼'}")
    
    val_size = int(len(texts) * args.val_split)
    train_texts = texts[:-val_size] if val_size > 0 else texts
    val_texts = texts[-val_size:] if val_size > 0 else []
    
    print(f"âœ“ è®­ç»ƒé›†: {len(train_texts)} æ ·æœ¬")
    print(f"âœ“ éªŒè¯é›†: {len(val_texts)} æ ·æœ¬")
    
    print("\nğŸ“š æ„å»ºè¯æ±‡è¡¨...")
    tokenizer = SimpleTokenizer()
    tokenizer.build_vocab(train_texts, min_freq=1)
    print(f"âœ“ è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ’» ä½¿ç”¨è®¾å¤‡: {device}")
    if device.type == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB")
    
    print("\nğŸ—ï¸  åˆ›å»ºæ¨¡å‹...")
    model = GCMLLM(
        vocab_size=tokenizer.vocab_size,
        d_model=args.d_model,
        n_heads=args.n_heads,
        n_layers=args.n_layers,
        d_ff=args.d_ff,
        max_len=args.max_len,
        dropout=args.dropout,
        use_rope=args.use_rope,
        use_flash=args.use_flash,
        use_gradient_checkpointing=args.use_gradient_checkpointing
    )
    model.tokenizer = tokenizer
    
    num_params = sum(p.numel() for p in model.parameters())
    print(f"âœ“ æ¨¡å‹å‚æ•°é‡: {num_params:,}")
    print(f"  æ¨¡å‹å¤§å°: ~{num_params * 4 / (1024**2):.2f} MB (float32)")
    
    config = {
        'd_model': args.d_model,
        'n_heads': args.n_heads,
        'n_layers': args.n_layers,
        'd_ff': args.d_ff,
        'max_len': args.max_len,
        'dropout': args.dropout,
        'vocab_size': tokenizer.vocab_size,
        'batch_size': args.batch_size,
        'epochs': args.epochs,
        'lr': args.lr,
        'augmentation_factor': args.augmentation_factor,
        'use_amp': args.use_amp,
        'save_dir': args.save_dir,
        'save_interval': args.save_interval,
        'log_interval': args.log_interval
    }
    
    os.makedirs(args.save_dir, exist_ok=True)
    
    config_path = os.path.join(args.save_dir, 'ultra_config.json')
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    print(f"âœ“ é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    
    print("\n" + "=" * 80)
    model, history = ultra_efficient_train(
        model=model,
        train_texts=train_texts,
        val_texts=val_texts,
        tokenizer=tokenizer,
        device=device,
        config=config,
        num_original_samples=len(train_texts)
    )
    
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
    final_path = os.path.join(args.save_dir, 'ultra_final_model.pt')
    torch.save({
        'model_state_dict': model.state_dict(),
        'tokenizer': tokenizer,
        'vocab_size': tokenizer.vocab_size,
        'config': config,
        'history': history
    }, final_path)
    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {final_path}")
    
    print("\n" + "=" * 80)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 80)
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æœ€ç»ˆéªŒè¯å›°æƒ‘åº¦: {history['val_ppls'][-1]:.2f}" if history['val_ppls'] else "N/A")
    print("=" * 80)


if __name__ == '__main__':
    main()

