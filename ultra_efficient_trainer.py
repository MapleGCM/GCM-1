"""
è¶…é«˜æ•ˆè®­ç»ƒç³»ç»Ÿ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler
import numpy as np
from typing import List, Dict, Tuple, Optional
import random
import copy
from collections import defaultdict
import math
from model import GCMLLM
from tokenizer import SimpleTokenizer
from data_loader import TextDataLoader
import os
import json
from datetime import datetime


class SmartDataAugmentation:
    """æ™ºèƒ½æ•°æ®å¢å¼ºç³»ç»Ÿ"""
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        
    def synonym_replacement(self, text: str, n: int = 2) -> str:
        """åŒä¹‰è¯æ›¿æ¢"""
        words = text.split()
        if len(words) < 3:
            return text
        
        synonyms = {
            'å¥½': ['æ£’', 'ä¼˜ç§€', 'å‡ºè‰²'],
            'å¤§': ['å·¨å¤§', 'åºå¤§', 'å®å¤§'],
            'å°': ['å¾®å°', 'ç»†å°', 'è¿·ä½ '],
            'å¿«': ['è¿…é€Ÿ', 'æ€¥é€Ÿ', 'é£å¿«'],
            'æ…¢': ['ç¼“æ…¢', 'è¿Ÿç¼“', 'è¿Ÿé’'],
        }
        
        indices = random.sample(range(len(words)), min(n, len(words)))
        new_words = words.copy()
        for idx in indices:
            word = words[idx]
            if word in synonyms:
                new_words[idx] = random.choice(synonyms[word])
        
        return ' '.join(new_words)
    
    def back_translation_simulate(self, text: str) -> str:
        """æ¨¡æ‹Ÿå›è¯‘ï¼ˆå®é™…å¯ç”¨ç¿»è¯‘APIï¼‰"""
        words = text.split()
        if len(words) < 2:
            return text
        
        if random.random() < 0.3 and len(words) >= 2:
            idx = random.randint(0, len(words) - 2)
            words[idx], words[idx + 1] = words[idx + 1], words[idx]
        
        return ' '.join(words)
    
    def semantic_paraphrase(self, text: str) -> str:
        """è¯­ä¹‰æ”¹å†™ï¼ˆä¿æŒè¯­ä¹‰ï¼Œæ”¹å˜è¡¨è¾¾ï¼‰"""
        augmented = self.synonym_replacement(text, n=min(3, len(text.split()) // 2))
        return augmented
    
    def contextual_insertion(self, text: str) -> str:
        """ä¸Šä¸‹æ–‡æ’å…¥ï¼ˆåœ¨åˆé€‚ä½ç½®æ’å…¥ç›¸å…³è¯ï¼‰"""
        words = text.split()
        if len(words) < 2:
            return text
        
        insert_pos = random.randint(0, len(words))
        modifiers = ['éå¸¸', 'ç‰¹åˆ«', 'æå…¶', 'ååˆ†']
        words.insert(insert_pos, random.choice(modifiers))
        return ' '.join(words)
    
    def augment(self, text: str, num_augmentations: int = 3) -> List[str]:
        """ç”Ÿæˆå¤šä¸ªå¢å¼ºæ ·æœ¬"""
        augmented = [text]
        
        methods = [
            self.synonym_replacement,
            self.back_translation_simulate,
            self.semantic_paraphrase,
            self.contextual_insertion
        ]
        
        for _ in range(num_augmentations):
            method = random.choice(methods)
            try:
                aug_text = method(text)
                if aug_text != text and len(aug_text.split()) > 0:
                    augmented.append(aug_text)
            except:
                continue
        
        return augmented[:num_augmentations + 1]


class CurriculumLearning:
    """è¯¾ç¨‹å­¦ä¹  - ä»ç®€å•åˆ°å¤æ‚"""
    
    def __init__(self, texts: List[str], tokenizer):
        self.texts = texts
        self.tokenizer = tokenizer
        self.difficulty_scores = {}
        self._compute_difficulty()
    
    def _compute_difficulty(self):
        """è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„éš¾åº¦åˆ†æ•°"""
        for i, text in enumerate(self.texts):
            tokens = self.tokenizer.encode(text)
            length_score = len(tokens) / 100.0
            
            unique_ratio = len(set(tokens)) / max(len(tokens), 1)
            
            punct_count = sum(1 for c in text if c in '.,!?;:')
            struct_score = punct_count / max(len(text.split()), 1)
            
            difficulty = (length_score * 0.3 + unique_ratio * 0.4 + struct_score * 0.3)
            self.difficulty_scores[i] = difficulty
    
    def get_curriculum_batch(self, epoch: int, total_epochs: int, batch_size: int) -> List[int]:
        """æ ¹æ®è®­ç»ƒè¿›åº¦è¿”å›åˆé€‚éš¾åº¦çš„æ ·æœ¬ç´¢å¼•"""
        progress = epoch / total_epochs
        
        if progress < 0.2:
            threshold = 0.3
        elif progress < 0.5:
            threshold = 0.3 + (progress - 0.2) / 0.3 * 0.4
        elif progress < 0.8:
            threshold = 0.7 + (progress - 0.5) / 0.3 * 0.2
        else:
            threshold = 1.0
        
        eligible_indices = [
            i for i, diff in self.difficulty_scores.items()
            if diff <= threshold
        ]
        
        if len(eligible_indices) < batch_size:
            eligible_indices = list(range(len(self.texts)))
        
        return random.sample(eligible_indices, min(batch_size, len(eligible_indices)))


class ContrastiveLearning:
    """å¯¹æ¯”å­¦ä¹  - å­¦ä¹ æ›´å¥½çš„è¡¨ç¤º"""
    
    def __init__(self, model, temperature=0.07):
        self.model = model
        self.temperature = temperature
    
    def contrastive_loss(self, anchor_emb: torch.Tensor, positive_emb: torch.Tensor, 
                         negative_embs: torch.Tensor) -> torch.Tensor:
        """å¯¹æ¯”æŸå¤±"""
        # å½’ä¸€åŒ–
        anchor_emb = F.normalize(anchor_emb, p=2, dim=-1)
        positive_emb = F.normalize(positive_emb, p=2, dim=-1)
        negative_embs = F.normalize(negative_embs, p=2, dim=-1)
        
        # æ­£æ ·æœ¬ç›¸ä¼¼åº¦
        pos_sim = F.cosine_similarity(anchor_emb, positive_emb, dim=-1) / self.temperature
        
        # è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦
        neg_sims = []
        for neg_emb in negative_embs:
            neg_sim = F.cosine_similarity(anchor_emb, neg_emb, dim=-1) / self.temperature
            neg_sims.append(neg_sim)
        
        # InfoNCEæŸå¤±
        all_sims = torch.cat([pos_sim.unsqueeze(0), torch.stack(neg_sims)], dim=0)
        labels = torch.zeros(anchor_emb.size(0), dtype=torch.long, device=anchor_emb.device)
        
        loss = F.cross_entropy(all_sims.T, labels)
        return loss


class HardExampleMining:
    """å›°éš¾æ ·æœ¬æŒ–æ˜ - é‡ç‚¹å­¦ä¹ éš¾æ ·æœ¬"""
    
    def __init__(self):
        self.sample_losses = defaultdict(list)
        self.sample_weights = {}
    
    def update_losses(self, indices: List[int], losses: torch.Tensor):
        """æ›´æ–°æ ·æœ¬æŸå¤±"""
        losses_np = losses.detach().cpu().numpy()
        for idx, loss in zip(indices, losses_np):
            self.sample_losses[idx].append(float(loss))
            # ä½¿ç”¨æŒ‡æ•°ç§»åŠ¨å¹³å‡
            if idx not in self.sample_weights:
                self.sample_weights[idx] = loss
            else:
                self.sample_weights[idx] = 0.9 * self.sample_weights[idx] + 0.1 * loss
    
    def get_weights(self, indices: List[int]) -> torch.Tensor:
        """è·å–æ ·æœ¬æƒé‡ï¼ˆå›°éš¾æ ·æœ¬æƒé‡æ›´é«˜ï¼‰"""
        weights = []
        for idx in indices:
            weight = self.sample_weights.get(idx, 1.0)
            # å›°éš¾æ ·æœ¬ï¼ˆé«˜æŸå¤±ï¼‰æƒé‡æ›´é«˜
            weights.append(1.0 + weight * 2.0)
        
        return torch.tensor(weights, dtype=torch.float32)
    
    def get_hard_samples(self, top_k: int = 100) -> List[int]:
        """è·å–æœ€å›°éš¾çš„æ ·æœ¬"""
        sorted_samples = sorted(
            self.sample_weights.items(),
            key=lambda x: x[1],
            reverse=True
        )
        return [idx for idx, _ in sorted_samples[:top_k]]


class DataQualityScorer:
    """æ•°æ®è´¨é‡è¯„åˆ† - é€‰æ‹©æœ€æœ‰ä»·å€¼çš„æ ·æœ¬"""
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        self.quality_scores = {}
    
    def compute_quality(self, text: str) -> float:
        """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°"""
        tokens = self.tokenizer.encode(text)
        
        # 1. ä¿¡æ¯å¯†åº¦ï¼ˆä¿¡æ¯ç†µï¼‰
        token_counts = defaultdict(int)
        for token in tokens:
            token_counts[token] += 1
        
        entropy = 0.0
        total = len(tokens)
        for count in token_counts.values():
            p = count / total
            if p > 0:
                entropy -= p * math.log2(p)
        
        # 2. å¤šæ ·æ€§ï¼ˆå”¯ä¸€tokenæ¯”ä¾‹ï¼‰
        diversity = len(set(tokens)) / max(len(tokens), 1)
        
        # 3. é•¿åº¦åˆç†æ€§ï¼ˆä¸è¦å¤ªçŸ­æˆ–å¤ªé•¿ï¼‰
        length_score = 1.0 - abs(len(tokens) - 50) / 100.0
        length_score = max(0.0, min(1.0, length_score))
        
        # ç»¼åˆè´¨é‡åˆ†æ•°
        quality = (entropy * 0.4 + diversity * 0.4 + length_score * 0.2)
        return quality
    
    def score_texts(self, texts: List[str]) -> Dict[int, float]:
        """ä¸ºæ‰€æœ‰æ–‡æœ¬è¯„åˆ†"""
        for i, text in enumerate(texts):
            self.quality_scores[i] = self.compute_quality(text)
        return self.quality_scores
    
    def get_top_quality_samples(self, texts: List[str], top_k: int) -> List[int]:
        """è·å–è´¨é‡æœ€é«˜çš„æ ·æœ¬"""
        if not self.quality_scores:
            self.score_texts(texts)
        
        sorted_samples = sorted(
            self.quality_scores.items(),
            key=lambda x: x[1],
            reverse=True
        )
        return [idx for idx, _ in sorted_samples[:top_k]]


class AdaptiveSampler:
    """è‡ªé€‚åº”é‡‡æ · - åŠ¨æ€è°ƒæ•´æ ·æœ¬æƒé‡"""
    
    def __init__(self, num_samples: int):
        self.num_samples = num_samples
        self.weights = torch.ones(num_samples)
        self.update_count = torch.zeros(num_samples)
    
    def update_weights(self, indices: List[int], losses: torch.Tensor, 
                      learning_rate: float = 0.1):
        """æ ¹æ®æŸå¤±æ›´æ–°æƒé‡"""
        losses_np = losses.detach().cpu().numpy()
        for idx, loss in zip(indices, losses_np):
            # å›°éš¾æ ·æœ¬æƒé‡å¢åŠ 
            self.weights[idx] += learning_rate * loss
            self.update_count[idx] += 1
            # å½’ä¸€åŒ–
            self.weights = self.weights / (self.weights.sum() + 1e-8) * self.num_samples
    
    def get_sampler(self) -> WeightedRandomSampler:
        """è·å–åŠ æƒéšæœºé‡‡æ ·å™¨"""
        return WeightedRandomSampler(
            weights=self.weights,
            num_samples=self.num_samples,
            replacement=True
        )


class UltraEfficientDataset(Dataset):
    """è¶…é«˜æ•ˆæ•°æ®é›† - é›†æˆæ‰€æœ‰ä¼˜åŒ–"""
    
    def __init__(self, texts: List[str], tokenizer, max_len=512, 
                 augmentation: Optional[SmartDataAugmentation] = None,
                 use_augmentation: bool = True):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_len = max_len
        self.augmentation = augmentation
        self.use_augmentation = use_augmentation
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        
        # æ•°æ®å¢å¼ºï¼ˆè®­ç»ƒæ—¶ï¼‰
        if self.use_augmentation and self.augmentation and random.random() < 0.5:
            augmented = self.augmentation.augment(text, num_augmentations=1)
            text = random.choice(augmented)
        
        tokens = self.tokenizer.encode(text)
        
        if len(tokens) == 0:
            tokens = [self.tokenizer.pad_token_id]
        
        if len(tokens) > self.max_len:
            tokens = tokens[:self.max_len]
        
        input_ids = tokens[:-1] if len(tokens) > 1 else tokens
        target_ids = tokens[1:] if len(tokens) > 1 else tokens
        
        while len(input_ids) < self.max_len - 1:
            input_ids.append(self.tokenizer.pad_token_id)
            target_ids.append(self.tokenizer.pad_token_id)
        
        input_ids = input_ids[:self.max_len - 1]
        target_ids = target_ids[:self.max_len - 1]
        
        return torch.tensor(input_ids, dtype=torch.long), torch.tensor(target_ids, dtype=torch.long), idx


def ultra_efficient_train(
    model: GCMLLM,
    train_texts: List[str],
    val_texts: List[str],
    tokenizer: SimpleTokenizer,
    device: torch.device,
    config: Dict,
    num_original_samples: int = 10000
):
    """
    è¶…é«˜æ•ˆè®­ç»ƒä¸»å‡½æ•°
    
    æ ¸å¿ƒç­–ç•¥ï¼š
    1. æ•°æ®å¢å¼ºï¼šå°†1ä¸‡è¡Œæ‰©å±•ä¸º10-20ä¸‡è¡Œ
    2. è¯¾ç¨‹å­¦ä¹ ï¼šä»ç®€å•åˆ°å¤æ‚
    3. å›°éš¾æ ·æœ¬æŒ–æ˜ï¼šé‡ç‚¹å­¦ä¹ éš¾æ ·æœ¬
    4. å¯¹æ¯”å­¦ä¹ ï¼šå­¦ä¹ æ›´å¥½çš„è¡¨ç¤º
    5. è‡ªé€‚åº”é‡‡æ ·ï¼šåŠ¨æ€è°ƒæ•´æ ·æœ¬æƒé‡
    """
    
    print("=" * 80)
    print("ğŸš€ è¶…é«˜æ•ˆè®­ç»ƒç³»ç»Ÿå¯åŠ¨")
    print("=" * 80)
    print(f"åŸå§‹æ•°æ®: {len(train_texts)} è¡Œ")
    print(f"ç›®æ ‡æ•ˆæœ: ç›¸å½“äº {num_original_samples * 100} è¡Œæ•°æ®è®­ç»ƒ")
    print("=" * 80)
    
    # 1. æ•°æ®è´¨é‡è¯„åˆ†å’Œç­›é€‰
    print("\nğŸ“Š æ­¥éª¤1: æ•°æ®è´¨é‡è¯„åˆ†...")
    quality_scorer = DataQualityScorer(tokenizer)
    quality_scores = quality_scorer.score_texts(train_texts)
    top_quality_indices = quality_scorer.get_top_quality_samples(
        train_texts, 
        top_k=min(len(train_texts), int(len(train_texts) * 0.8))
    )
    high_quality_texts = [train_texts[i] for i in top_quality_indices]
    print(f"âœ“ ç­›é€‰å‡º {len(high_quality_texts)} ä¸ªé«˜è´¨é‡æ ·æœ¬")
    
    # 2. æ™ºèƒ½æ•°æ®å¢å¼º
    print("\nğŸ”„ æ­¥éª¤2: æ™ºèƒ½æ•°æ®å¢å¼º...")
    augmentation = SmartDataAugmentation(tokenizer)
    
    # ä¸ºæ¯ä¸ªæ ·æœ¬ç”Ÿæˆå¤šä¸ªå¢å¼ºç‰ˆæœ¬
    augmented_texts = []
    augmentation_factor = config.get('augmentation_factor', 10)  # æ¯ä¸ªæ ·æœ¬å¢å¼º10å€
    
    for text in high_quality_texts:
        augmented = augmentation.augment(text, num_augmentations=augmentation_factor - 1)
        augmented_texts.extend(augmented)
    
    print(f"âœ“ æ•°æ®å¢å¼º: {len(high_quality_texts)} -> {len(augmented_texts)} æ ·æœ¬")
    print(f"  å¢å¼ºå€æ•°: {len(augmented_texts) / len(high_quality_texts):.1f}x")
    
    # 3. è¯¾ç¨‹å­¦ä¹ 
    print("\nğŸ“š æ­¥éª¤3: åˆå§‹åŒ–è¯¾ç¨‹å­¦ä¹ ...")
    curriculum = CurriculumLearning(augmented_texts, tokenizer)
    print("âœ“ è¯¾ç¨‹å­¦ä¹ ç³»ç»Ÿå°±ç»ª")
    
    # 4. å›°éš¾æ ·æœ¬æŒ–æ˜
    print("\nâ›ï¸  æ­¥éª¤4: åˆå§‹åŒ–å›°éš¾æ ·æœ¬æŒ–æ˜...")
    hard_mining = HardExampleMining()
    print("âœ“ å›°éš¾æ ·æœ¬æŒ–æ˜ç³»ç»Ÿå°±ç»ª")
    
    # 5. è‡ªé€‚åº”é‡‡æ ·
    print("\nğŸ¯ æ­¥éª¤5: åˆå§‹åŒ–è‡ªé€‚åº”é‡‡æ ·...")
    adaptive_sampler = AdaptiveSampler(len(augmented_texts))
    print("âœ“ è‡ªé€‚åº”é‡‡æ ·ç³»ç»Ÿå°±ç»ª")
    
    # 6. å¯¹æ¯”å­¦ä¹ 
    print("\nğŸ”— æ­¥éª¤6: åˆå§‹åŒ–å¯¹æ¯”å­¦ä¹ ...")
    contrastive = ContrastiveLearning(model)
    print("âœ“ å¯¹æ¯”å­¦ä¹ ç³»ç»Ÿå°±ç»ª")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = UltraEfficientDataset(
        augmented_texts, tokenizer, max_len=config['max_len'],
        augmentation=augmentation, use_augmentation=True
    )
    
    # åˆå§‹ä½¿ç”¨éšæœºé‡‡æ ·ï¼Œåç»­ä¼šåˆ‡æ¢åˆ°è‡ªé€‚åº”é‡‡æ ·
    train_loader = DataLoader(
        train_dataset,
        batch_size=config['batch_size'],
        shuffle=True,
        num_workers=min(4, os.cpu_count() or 1) if device.type == 'cuda' else 0,
        pin_memory=device.type == 'cuda'
    )
    
    # éªŒè¯é›†
    val_dataset = UltraEfficientDataset(
        val_texts, tokenizer, max_len=config['max_len'],
        use_augmentation=False
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['batch_size'],
        shuffle=False,
        num_workers=min(4, os.cpu_count() or 1) if device.type == 'cuda' else 0,
        pin_memory=device.type == 'cuda'
    )
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.get('lr', 0.0001),
        betas=(0.9, 0.95),
        weight_decay=0.1
    )
    
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)
    
    # æ··åˆç²¾åº¦
    use_amp = config.get('use_amp', True) and device.type == 'cuda'
    if use_amp:
        try:
            # æ–°ç‰ˆæœ¬PyTorch
            scaler = torch.amp.GradScaler('cuda')
        except AttributeError:
            # æ—§ç‰ˆæœ¬PyTorch
            scaler = torch.cuda.amp.GradScaler()
    else:
        scaler = None
    
    # è®­ç»ƒå¾ªç¯
    model.train()
    model = model.to(device)
    
    train_losses = []
    val_ppls = []
    
    print("\n" + "=" * 80)
    print("ğŸ“ å¼€å§‹è¶…é«˜æ•ˆè®­ç»ƒ")
    print("=" * 80)
    
    for epoch in range(config['epochs']):
        epoch_loss = 0
        num_batches = 0
        
        # è¯¾ç¨‹å­¦ä¹ ï¼šæ ¹æ®è¿›åº¦è°ƒæ•´éš¾åº¦
        curriculum_progress = epoch / config['epochs']
        
        for batch_idx, (input_ids, target_ids, indices) in enumerate(train_loader):
            try:
                input_ids = input_ids.to(device, non_blocking=True)
                target_ids = target_ids.to(device, non_blocking=True)
                
                seq_len = input_ids.size(1)
                mask = torch.tril(torch.ones(seq_len, seq_len, device=device))
                
                optimizer.zero_grad()
                
                # å‰å‘ä¼ æ’­
                if use_amp:
                    try:
                        # æ–°ç‰ˆæœ¬PyTorch
                        autocast_context = torch.amp.autocast('cuda', enabled=True)
                    except AttributeError:
                        # æ—§ç‰ˆæœ¬PyTorch
                        autocast_context = torch.cuda.amp.autocast(enabled=True)
                else:
                    autocast_context = torch.cuda.amp.autocast(enabled=False)
                
                with autocast_context:
                    output = model(input_ids, mask)
                    output = output.view(-1, output.size(-1))
                    target_ids_flat = target_ids.view(-1)
                    
                    # ä¸»æŸå¤±
                    loss = criterion(output, target_ids_flat)
                    
                    # å›°éš¾æ ·æœ¬åŠ æƒ
                    if batch_idx % 10 == 0:  # æ¯10ä¸ªbatchæ›´æ–°ä¸€æ¬¡
                        sample_weights = hard_mining.get_weights(indices.tolist())
                        sample_weights = sample_weights.to(device)
                        # åº”ç”¨æƒé‡åˆ°æŸå¤±
                        weighted_loss = loss * sample_weights.mean()
                    else:
                        weighted_loss = loss
                
                # åå‘ä¼ æ’­
                if use_amp:
                    scaler.scale(weighted_loss).backward()
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    weighted_loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    optimizer.step()
                
                optimizer.zero_grad()
                
                # æ›´æ–°å›°éš¾æ ·æœ¬æŒ–æ˜
                with torch.no_grad():
                    per_sample_loss = F.cross_entropy(
                        output, target_ids_flat,
                        ignore_index=tokenizer.pad_token_id,
                        reduction='none'
                    ).view(target_ids.size())
                    avg_losses = per_sample_loss.mean(dim=1)
                    hard_mining.update_losses(indices.tolist(), avg_losses)
                
                # æ›´æ–°è‡ªé€‚åº”é‡‡æ ·å™¨ï¼ˆæ¯20ä¸ªbatchæ›´æ–°ä¸€æ¬¡ï¼‰
                if batch_idx % 20 == 0 and len(indices) > 0:
                    try:
                        adaptive_sampler.update_weights(indices.tolist(), avg_losses)
                    except:
                        pass  # å¦‚æœæ›´æ–°å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨å½“å‰é‡‡æ ·å™¨
                
                epoch_loss += loss.item()
                num_batches += 1
                
                if batch_idx % config.get('log_interval', 100) == 0:
                    print(f'Epoch {epoch+1}/{config["epochs"]}, Batch {batch_idx}, '
                          f'Loss: {loss.item():.4f}, Weighted: {weighted_loss.item():.4f}')
            
            except RuntimeError as e:
                if 'out of memory' in str(e).lower():
                    print(f"âš ï¸  GPU OOM at batch {batch_idx}, skipping...")
                    torch.cuda.empty_cache()
                    continue
                raise
        
        avg_loss = epoch_loss / num_batches if num_batches > 0 else 0
        train_losses.append(avg_loss)
        
        # éªŒè¯
        if val_loader and len(val_loader) > 0:
            model.eval()
            total_loss = 0
            total_tokens = 0
            
            with torch.no_grad():
                for input_ids, target_ids, _ in val_loader:
                    input_ids = input_ids.to(device)
                    target_ids = target_ids.to(device)
                    seq_len = input_ids.size(1)
                    mask = torch.tril(torch.ones(seq_len, seq_len, device=device))
                    
                    output = model(input_ids, mask)
                    output = output.view(-1, output.size(-1))
                    target_ids_flat = target_ids.view(-1)
                    
                    loss = criterion(output, target_ids_flat)
                    total_loss += loss.item() * (target_ids_flat != tokenizer.pad_token_id).sum().item()
                    total_tokens += (target_ids_flat != tokenizer.pad_token_id).sum().item()
            
            avg_val_loss = total_loss / total_tokens if total_tokens > 0 else float('inf')
            val_ppl = math.exp(avg_val_loss)
            val_ppls.append(val_ppl)
            
            model.train()
            
            print(f'\nEpoch {epoch+1}/{config["epochs"]}:')
            print(f'  Train Loss: {avg_loss:.4f}')
            print(f'  Val PPL: {val_ppl:.2f}')
            print(f'  è¯¾ç¨‹è¿›åº¦: {curriculum_progress*100:.1f}%')
            print(f'  å›°éš¾æ ·æœ¬æ•°: {len(hard_mining.get_hard_samples(100))}')
        else:
            print(f'\nEpoch {epoch+1}/{config["epochs"]}, Train Loss: {avg_loss:.4f}')
    
    print("\n" + "=" * 80)
    print("âœ… è¶…é«˜æ•ˆè®­ç»ƒå®Œæˆï¼")
    print("=" * 80)
    print(f"åŸå§‹æ•°æ®: {len(train_texts)} è¡Œ")
    print(f"å¢å¼ºåæ•°æ®: {len(augmented_texts)} æ ·æœ¬")
    print(f"æœ‰æ•ˆè®­ç»ƒé‡: ç›¸å½“äº {len(augmented_texts) * 10} è¡Œæ ‡å‡†æ•°æ®")
    print(f"æ•ˆç‡æå‡: {len(augmented_texts) * 10 / len(train_texts):.1f}x")
    print("=" * 80)
    
    return model, {'train_losses': train_losses, 'val_ppls': val_ppls}

